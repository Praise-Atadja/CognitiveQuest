# -*- coding: utf-8 -*-
"""preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yMuze41iVyHNc_iTuK_wL9aIFxaOfDzc

# **PROJECT NAME: CognitiveQuest**
---
(***This project is to predict the possibility of autism***)

# **Data Cleaning, Feature Engineering, Transformation**

Load Datasets
"""

import pandas as pd

def load_autism_data():
    df_children = pd.read_csv('/content/sample_data/Autism-Child-Data.csv')
    df_adults = pd.read_csv('/content/sample_data/Autism-Adult-Data.csv')

    return df_children, df_adults

# Load the data
df_children, df_adults = load_autism_data()

"""***Current Format of the Data:***

The current format of the data varies slightly across the two datasets:  children and adults. Each dataset consists of multiple instances (cases) where each instance represents an individual, and each instance contains several attributes/features related to autism screening, such as behavioral responses, demographic information, and diagnostic outcomes. The attributes include categorical, binary, and continuous variables, and the datasets are intended for classification tasks in the medical, health, and social science domains.

***Outline of Planned Transformation to Unified Format:***

To achieve a unified format suitable for analysis and modeling, the following steps will be taken:

- Ensure that each dataset has the same set of features with consistent naming conventions.
- Merge the datasets into a single dataset, ensuring compatibility of features and target variables.
- Address missing values and inconsistencies in the data.
- Standardize categorical variables if necessary.

Check Features
"""

def enumerate_features(df, dataset_name):
    print(f"Features included in the {dataset_name} dataset:")
    for feature in df.columns:
        print("-", feature)

def check_rows_and_columns(df, dataset_name):
    print(f"{dataset_name} - rows: {df.shape[0]}, columns: {df.shape[1]}")

def inspect_structure(df, dataset_name):
    print(f"{dataset_name}:")
    print(df.head())  # Display the first few rows

# Enumerate features included in the child dataset
enumerate_features(df_children, "child")

# Enumerate features included in the adult dataset
enumerate_features(df_adults, "adult")

# Check rows and columns for both datasets
check_rows_and_columns(df_children, "children")
check_rows_and_columns(df_adults, "adults")

# Inspect the structure
inspect_structure(df_children, "children")
inspect_structure(df_adults, "adults")

"""Data Overview"""

def data_overview(df, dataset_name):
    print(f"\n{dataset_name} Data Overview:")
    print(df.info())
    print("\nSummary Statistics:")
    print(df.describe())

# Data overview for child dataset
data_overview(df_children, "Children")

# Data overview for adult dataset
data_overview(df_adults, "Adults")

"""Analyze distributions of numerical features"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_numerical_feature_distributions(df_children, df_adults):
    # Analysis for Children
    print("\nAnalysis for Children:")
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df_children[['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                                   'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age']], kde=True)
    plt.title('Histogram of Numerical Variables for Children')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.legend(['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age'], loc='upper right')
    plt.show()

    # Analysis for Adults
    print("\nAnalysis for Adults:")
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df_adults[['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                                 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age']], kde=True)
    plt.title('Histogram of Numerical Variables for Adults')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.legend(['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age'], loc='upper right')
    plt.show()

# Call the function with the child and adult datasets
plot_numerical_feature_distributions(df_children, df_adults)

"""Analyze correlations"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_correlation_matrices(df_children, df_adults):
    # Correlation Matrix for Children
    correlation_matrix_children = df_children.corr()
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix_children, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Correlation Matrix for Children')
    plt.show()

    # Correlation Matrix for Adults (using the same variable as for children)
    correlation_matrix_adults = df_adults.corr()
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix_adults, annot=True, cmap='coolwarm', fmt=".2f")  # using the same variable
    plt.title('Correlation Matrix for Adults')
    plt.show()

# Call the function with the child and adult datasets
plot_correlation_matrices(df_children, df_adults)

"""Merge both datasets"""

def merge_datasets(df1, df2):
    try:
        merged_data = pd.concat([df1, df2], ignore_index=True)
        print("Datasets concatenated successfully.")
        return merged_data
    except ValueError as e:
        print("Error:", e)
        print("Column mismatch detected. Check column names and data types in both datasets.")
        return None

# Merge datasets
merged_data = merge_datasets(df_adults, df_children)

# If merged_data is not None, you can continue working with the merged dataset
if merged_data is not None:
    print("Merged dataset shape:", merged_data.shape)
    print("Columns in merged dataset:", merged_data.columns)

"""Rename misspelled columns"""

def rename_columns(df):
    renamed_columns = {
        'austim': 'autism',
        'jundice': 'jaundice',
        'contry_of_res': 'country_of_res',

    }
    df = df.rename(columns=renamed_columns)
    return df

# Rename misspelled column names in the merged dataset
merged_data = rename_columns(merged_data)
merged_data.head()

"""Rename misspelled responses"""

def rename_misspelled_responses(df):
    # Rename misspelled responses
    df['relation'].replace('self', 'Self', inplace=True)
    return df

# Call the function with the merged dataset
merged_data = rename_misspelled_responses(merged_data)

"""Check for missing data in merged dataset"""

def check_missing_data(df, dataset_name):
    missing_data = df.isna().sum()
    print(f"Missing data in {dataset_name} dataset:")
    print(missing_data)

# Check for missing data in the merged dataset
check_missing_data(merged_data, "merged")

"""Handling missing data"""

def handle_missing_data(df):
    # Replace '?' with 'Unknown' in the 'ethnicity' column
    df['ethnicity'].replace('?', 'Unknown', inplace=True)

    # Replace '?' with 'Unknown' in the 'relation' column
    df['relation'].replace('?', 'Unknown', inplace=True)

    return df

merged_data = handle_missing_data(merged_data)

"""Replacing non-numerical values in age column"""

def replace_non_numeric_with_nan(df):
    df['age'] = pd.to_numeric(df['age'], errors='coerce')
    return df

# Replace non-numeric values with NaN in the merged dataset
merged_data = replace_non_numeric_with_nan(merged_data)

"""Finding Outliers in both datasets"""

def find_outliers_age(df, dataset_name):
    max_age = df['age'].max()
    min_age = df['age'].min()

    print(f'Maximum age in {dataset_name} dataset:', max_age)
    print(f'Minimum age in {dataset_name} dataset:', min_age)

# Outliers for Age in Children
find_outliers_age(merged_data, "Merged Dataset")

# Removing row with maximum age from merged dataset
max_age_row_index = merged_data['age'].idxmax()
print("Removing row:", max_age_row_index, "as the respondent has an age of", merged_data.loc[max_age_row_index, 'age'], "which is impossible.")
merged_data.drop(index=max_age_row_index, inplace=True)
merged_data.reset_index(drop=True, inplace=True)

"""Drop Unsed columns"""

def drop_unused_columns(df):
    # Drop unused columns
    unused_columns = [ 'id', 'age_desc']
    df.drop(columns=unused_columns, inplace=True)

    # Print messages about dropped columns
    print("Columns 'id' and 'age_desc' are considered irrelevant and 'age_range' is standard. Dropping them.")

    return df

# Drop unused columns from the merged dataset
merged_data = drop_unused_columns(merged_data)
merged_data.head()

"""Analyzing Each Column

Ethnicity
"""

import matplotlib.pyplot as plt

def plot_ethnicity_percentages(df):
    # Group the data by ethnicity and calculate the size of each group
    ethnicity_counts = df.groupby('ethnicity').size()

    # Calculate the total number of cases
    total_cases = ethnicity_counts.sum()

    # Calculate the percentage of cases for each ethnicity
    ethnicity_percentages = (ethnicity_counts / total_cases) * 100

    # Plot the percentages
    plt.figure(figsize=(10, 6))
    ethnicity_percentages.plot(kind='bar', color='skyblue')
    plt.title('Percentage of Cases by Ethnicity')
    plt.xlabel('Ethnicity')
    plt.ylabel('Percentage')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Call the function with the merged dataset
plot_ethnicity_percentages(merged_data)

"""Conclusion : White Europeans have most cases of ASD around the world
followed by asians.

Gender
"""

import matplotlib.pyplot as plt

def plot_cases_by_gender(df):
    # Group the data by gender and calculate the size of each group
    gender_counts = df.groupby('gender').size()

    # Plot the number of cases by gender
    gender_counts.plot(kind='bar')

    # Adding labels and title to the plot
    plt.xlabel('Gender')
    plt.ylabel('Number of Cases')
    plt.title('Cases by Gender')
    plt.xticks(rotation=360)

    plt.show()

# Call the function with the merged dataset
plot_cases_by_gender(merged_data)

"""Conclusion : Males are more prone to ASD in comparision to Females

Jaundice
"""

import matplotlib.pyplot as plt

def plot_cases_by_jaundice(df):
    # Group the data by jaundice and calculate the size of each group
    jaundice_counts = df.groupby('jaundice').size()

    # Plot the number of ASD cases by jaundice
    jaundice_counts.plot(kind='bar')

    # Adding labels and title to the plot
    plt.xlabel('Jaundice')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Jaundice')
    plt.xticks(rotation=360)

    plt.show()

# Call the function with the merged dataset
plot_cases_by_jaundice(merged_data)

"""Conclusion : Having jundice does not determine you will have autism.

Relation
"""

import matplotlib.pyplot as plt

def plot_cases_by_relation(df):
    # Aggregate duplicate labels by counting the number of occurrences
    relation_counts = df.groupby('relation').size()

    # Plot the aggregated counts
    relation_counts.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.xlabel('Relation')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Relation')
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
    plt.tight_layout()
    plt.show()

# Call the function with the merged dataset
plot_cases_by_relation(merged_data)

"""Conclusion : Most ASD cases around the world do not have their family members with ASD that means ASD is not a genetic disease.

Age
"""

import numpy as np
import matplotlib.pyplot as plt

def plot_cases_by_age_range(df):
    # Create bins from 4 to 64 with a step size of 5
    age_bins = np.arange(4, 65, 5)

    # Create age labels for the bins
    age_labels = [f'{age}-{age+4}' for age in age_bins[:-1]]

    # Discretize age into bins and assign labels
    df['age_range'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)

    # Plotting the number of cases by age range
    df.groupby('age_range').size().plot(kind='bar', color='skyblue', edgecolor='black')
    plt.xlabel('Age Range')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Age Range')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Call the function with the merged dataset
plot_cases_by_age_range(merged_data)

"""Conclusion: Most ASD cases are highest with persons between 4-8 years and 19-23 years

Qchat
"""

import matplotlib.pyplot as plt

def plot_cases_by_score(df):
    # Get the counts of positive scores for each question
    score_counts = df[['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']].sum()

    # Create a bar plot for each score
    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))
    fig.suptitle('ASD Cases by Score', y=1.05)

    for i, (score, ax) in enumerate(zip(score_counts.index, axes.flatten())):
        ax.bar(['Positive', 'Negative'], [score_counts[score], len(df) - score_counts[score]], color=['skyblue', 'lightcoral'])
        ax.set_title(f'Score {i+1}')
        ax.set_ylabel('Number of Cases')

    plt.tight_layout()
    plt.show()

# Call the function with the merged dataset
plot_cases_by_score(merged_data)

"""Country"""

import matplotlib.pyplot as plt

def plot_cases_by_country(df):
    # Group the data by country of residence and calculate the size of each group
    cases_by_country = df.groupby('country_of_res').size().sort_values(ascending=False)

    # Plotting the number of cases by country
    plt.figure(figsize=(12, 6))
    cases_by_country.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.xlabel('Country')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Country')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

# Call the function with the merged dataset
plot_cases_by_country(merged_data)

"""Conclusion more residents of the United States have more ASD cases.

Results
"""

import matplotlib.pyplot as plt

def plot_cases_by_result(df):
    # Plotting the number of cases against the 'result' column
    plt.figure(figsize=(10, 6))
    df['result'].value_counts().sort_index().plot(kind='bar', color='skyblue', edgecolor='black')
    plt.xlabel('Result')
    plt.ylabel('Number of Cases')
    plt.title('Number of Cases by Result')
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

# Call the function with the merged dataset
plot_cases_by_result(merged_data)

"""Used_app_before"""

import matplotlib.pyplot as plt

def plot_cases_by_used_app_before(df):
    # Group the data by 'used_app_before' and calculate the size of each group
    cases_by_used_app_before = df.groupby('used_app_before').size()

    # Plotting the number of cases by whether the individuals used the app before or not
    plt.figure(figsize=(6, 6))
    cases_by_used_app_before.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.xlabel('Used App Before')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Used App Before')
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

# Call the function with the merged dataset
plot_cases_by_used_app_before(merged_data)

"""Feature Relevance"""

import matplotlib.pyplot as plt

# Define features
features_numerical = ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age']
features_categorical = ['gender', 'ethnicity', 'jaundice', 'autism', 'country_of_res', 'relation', 'Class/ASD']

# Combine all features
all_features = features_numerical + features_categorical

# Calculate feature importance or relevance scores (you need to replace this with actual feature importance values)
feature_importance = {feature: importance_score for feature, importance_score in zip(all_features, range(len(all_features)))}

# Define colormap
cmap = plt.get_cmap('viridis')  # You can choose any other colormap

# Plot total cases for each feature with colors based on importance
plt.figure(figsize=(12, 8))
for i, feature in enumerate(all_features):
    total_cases = merged_data.groupby(feature).size()
    color = cmap(feature_importance[feature] / len(all_features))  # Normalize importance to [0, 1] for colormap
    total_cases.plot(kind='bar', color=color, alpha=0.7, label=feature)  # Use alpha to make bars transparent for better visualization
plt.title('Total Cases by Feature (Colored by Importance)')
plt.xlabel('Feature Values')
plt.ylabel('Total Cases')
plt.legend()
plt.xticks(rotation=45)

# Add colorbar
sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=1))  # Normalize colormap to [0, 1]
sm.set_array([])  # Set an empty array since we only want colors
cbar = plt.colorbar(sm)
cbar.set_label('Feature Importance')

plt.tight_layout()
plt.show()

# Print out relevant features and their importance scores
print("Relevant features and their importance scores:")
for feature, importance_score in feature_importance.items():
    print(f"Feature: {feature}, Importance Score: {importance_score}")

"""One-Hot-Coding"""

def split_data(merged_data):
    # Extract features and target label
    features = merged_data.drop(columns=['Class/ASD'])
    target_label = merged_data['Class/ASD']

    return features, target_label

features, target_label = split_data(merged_data)

from sklearn.preprocessing import MinMaxScaler

def min_max_scale_numerical_features(features, numerical_features):
    # Initialize MinMaxScaler
    scaler = MinMaxScaler()

    # Make a copy of features
    features_minmax_transformed = features.copy()

    # Apply Min-Max scaling to numerical features
    features_minmax_transformed[numerical_features] = scaler.fit_transform(features[numerical_features])

    return features_minmax_transformed

# Define numerical features
numerical_features = ['age', 'result']

# Call the function with features and numerical features
features_minmax_transformed = min_max_scale_numerical_features(features, numerical_features)

# Show an example of a record with scaling applied
print("Example of records with scaling applied:")
display(features_minmax_transformed.head(n=5))

def one_hot_encode(features_minmax_transform):
    # One-hot encode the features
    features_final = pd.get_dummies(features_minmax_transform)

    return features_final

# Call the function with features after Min-Max scaling
features_final = one_hot_encode(features_minmax_transformed)

# Display the first 5 rows of the one-hot encoded features
print("One-hot encoded features:")
display(features_final.head(5))

# Encode the 'Class/ASD' target label to numerical values (0 and 1)
merged_data_classes = target_label.apply(lambda x: 1 if x == 'YES' else 0)

# Print the number of features after one-hot encoding
encoded_features_count = len(features_final.columns)
print(f"{encoded_features_count} total features after one-hot encoding.")


print(features_final.columns.tolist())

"""Checking if categorical variables have been converted into numerical features, and all numerical features have been normalized"""

def plot_class_histogram(merged_data_classes):
    # Plot histogram with 10 bins
    plt.hist(merged_data_classes, bins=10)

    # Set x-axis limit from 0 to 1
    plt.xlim(0, 1)
    plt.title('Histogram of Class/ASD')
    plt.xlabel('Class/ASD from processed data')
    plt.ylabel('Frequency')
    plt.show()

# Call the function with the 'Class/ASD' target label
plot_class_histogram(merged_data_classes)

"""Splitting datasets"""

from sklearn.model_selection import train_test_split

def split_data(features_final, merged_data_classes):
    # Split combined dataset into features (X) and target variable (y)
    X = features_final
    y = merged_data_classes

    # Perform randomization and split into train, validation, and test sets
    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, stratify=y_train_val, random_state=42)

    return X_train, X_val, X_test, y_train, y_val, y_test

# Call the function to split the data
X_train, X_val, X_test, y_train, y_val, y_test = split_data(features_final, merged_data_classes)

print("Training set shape:", X_train.shape, y_train.shape)
print("Validation set shape:", X_val.shape, y_val.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

"""Saving data files"""

def save_datasets(X_train, y_train, X_val, y_val, X_test, y_test):
    # Save training dataset
    training_data_path = 'data/training_data.csv'
    X_train.to_csv(training_data_path, index=False)
    y_train.to_csv('data/training_labels.csv', index=False)

    # Save validation dataset
    validation_data_path = 'data/validation_data.csv'
    X_val.to_csv(validation_data_path, index=False)
    y_val.to_csv('data/validation_labels.csv', index=False)

    # Save testing dataset
    testing_data_path = 'data/testing_data.csv'
    X_test.to_csv(testing_data_path, index=False)
    y_test.to_csv('data/testing_labels.csv', index=False)

# Call the function with the datasets
save_datasets(X_train, y_train, X_val, y_val, X_test, y_test)