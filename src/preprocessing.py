# -*- coding: utf-8 -*-
"""preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z-CQO2g1eiJSqU06G6jCeALHVrLazqsc

# **PROJECT NAME: CognitiveQuest**
---
(***This project is to predict the possibility of autism***)

# **The Dataset**
**Short Description of the Data:**

The dataset contains information pertinent to the screening of autism spectrum disorder (ASD), comprising the following columns:

1. ID: Unique identifier for each patient
2. A1_Score to A10_Score: Scores derived from the Autism Spectrum Quotient (AQ) 10 item screening tool
3. age: Age of the patient in years
4. gender: Gender of the patient
5. ethnicity: Ethnicity of the patient
6. jaundice: Indicator of whether the patient experienced jaundice at birth
7. autism: Indication of whether an immediate family member has been diagnosed with autism
8. country_of_res: Country where the patient resides
9. used_app_before: Flag indicating if the patient has previously undergone a screening test
10. result: Aggregate score for AQ1-10 screening test
11. age_desc: Description of the patient's age
12. relation: Relationship of the individual who completed the test with the patient
13. Class/ASD: Binary classification of ASD presence, where 0 signifies No ASD and 1 indicates ASD presence (target column)

The dataset is divided into a training set (train.csv) and a test set (test.csv).

The primary objective is to forecast the probability of an individual having ASD based on the provided features, with the "Class/ASD" column serving as the target variable for prediction.

1. **Data Sources and Aggregation:**
    
***Data Sources***:

- **UC Irvine Machine Learning Repository (Autistic Spectrum Disorder Screening Data for Children)**:

This dataset contains information about individuals, including their scores on various screening questions for Autism Spectrum Disorder (ASD), demographic details such as age, gender, and ethnicity, as well as other factors like whether they have previously used ASD screening apps or if they have a family history of autism. Each individual is classified as either having ASD or not based on the screening results. [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/419/autistic+spectrum+disorder+screening+data+for+children)


- **UC Irvine Machine Learning Repository (Autistic Spectrum Disorder Screening Data for Adults)**:

This dataset contains information about adults and their scores on various screening questions for Autism Spectrum Disorder (ASD), along with demographic details such as age, gender, and ethnicity. It aims to facilitate the analysis of influential autistic traits and improve the classification of ASD cases. There are 704 instances and 21 attributes in the dataset, which includes missing values, and it is suitable for classification tasks in the medical, health, and social science domains. [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/426/autism+screening+adult)

***Necessity for Data Aggregation***

In reference to the provided data sources:

Data aggregation from multiple sources is necessary for comprehensive analysis as it allows for a broader coverage across different age groups (children and adults), enhances the sample size for statistical significance, validates findings through cross-verification, and addresses potential data gaps or biases present in individual datasets.

# **Data Cleaning, Feature Engineering, Transformation**

***Outline of Planned Transformation to Unified Format:***

To achieve a unified format suitable for analysis and modeling, the following steps will be taken:

- Ensure that each dataset has the same set of features with consistent naming conventions.
- Standardize categorical variables if necessary.
- Address missing values and inconsistencies in the data.
- Merge the datasets into a single dataset, ensuring compatibility of features and target variables.
"""

# Commented out IPython magic to ensure Python compatibility.
#Import Necessary Libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import scipy
import scipy.stats
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import classification_report
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from joblib import dump
from scipy.stats import mstats
from google.colab import drive
drive.mount('/content/drive')
import os
import xgboost as xgb
import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score, cohen_kappa_score, confusion_matrix
import joblib
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier

"""Load Datasets"""

#load data
def load_autism_data():
    # Load data
    df_children = pd.read_csv('/content/drive/MyDrive/Autism-Child-Data.csv')
    df_adults = pd.read_csv('/content/drive/MyDrive/Autism-Adult-Data.csv')

    return df_children, df_adults

    # Load the data
df_children, df_adults = load_autism_data()

"""***Current Format of the Data:***

The current format of the data varies slightly across the two datasets:  children and adults. Each dataset consists of multiple instances (cases) where each instance represents an individual, and each instance contains several attributes/features related to autism screening, such as behavioral responses, demographic information, and diagnostic outcomes. The attributes include categorical, binary, and continuous variables, and the datasets are intended for classification tasks in the medical, health, and social science domains.

***Outline of Planned Transformation to Unified Format:***

To achieve a unified format suitable for analysis and modeling, the following steps will be taken:

- Ensure that each dataset has the same set of features with consistent naming conventions.
- Merge the datasets into a single dataset, ensuring compatibility of features and target variables.
- Address missing values and inconsistencies in the data.
- Standardize categorical variables if necessary.

Check Features
"""

def enumerate_features(df, dataset_name):
    print(f"Features included in the {dataset_name} dataset:")
    for feature in df.columns:
        print("-", feature)

def check_rows_and_columns(df, dataset_name):
    print(f"{dataset_name} - rows: {df.shape[0]}, columns: {df.shape[1]}")

def inspect_structure(df, dataset_name):
    print(f"{dataset_name}:")
    print(df.head())  # Display the first few rows

# Enumerate features included in the child dataset
enumerate_features(df_children, "child")

# Enumerate features included in the adult dataset
enumerate_features(df_adults, "adult")

# Check rows and columns for both datasets
check_rows_and_columns(df_children, "children")
check_rows_and_columns(df_adults, "adults")

# Inspect the structure
inspect_structure(df_children, "children")
inspect_structure(df_adults, "adults")

"""Data Overview"""

def data_overview(df, dataset_name):
    print(f"\n{dataset_name} Data Overview:")
    print(df.info())
    print("\nSummary Statistics:")
    print(df.describe())

# Data overview for child dataset
data_overview(df_children, "Children")

# Data overview for adult dataset
data_overview(df_adults, "Adults")

"""Analyze distributions of numerical features"""

def plot_numerical_feature_distributions(df_children, df_adults):
    # Analysis for Children
    print("\nAnalysis for Children:")
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df_children[['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                                   'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age']], kde=True)
    plt.title('Histogram of Numerical Variables for Children')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.legend(['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age'], loc='upper right')
    plt.show()

    # Analysis for Adults
    print("\nAnalysis for Adults:")
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df_adults[['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                                 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age']], kde=True)
    plt.title('Histogram of Numerical Variables for Adults')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.legend(['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',
                'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age'], loc='upper right')
    plt.show()


plot_numerical_feature_distributions(df_children, df_adults)

"""Analyze correlations"""

def plot_correlation_matrices(df_children, df_adults):
    # Filter out non-numeric columns
    numeric_columns_children = df_children.select_dtypes(include=[np.number])
    numeric_columns_adults = df_adults.select_dtypes(include=[np.number])

    # Correlation Matrix for Children
    correlation_matrix_children = numeric_columns_children.corr()
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix_children, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Correlation Matrix for Children')
    plt.show()

    # Correlation Matrix for Adults (using the same variable as for children)
    correlation_matrix_adults = numeric_columns_adults.corr()
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix_adults, annot=True, cmap='coolwarm', fmt=".2f")  # using the same variable
    plt.title('Correlation Matrix for Adults')
    plt.show()


plot_correlation_matrices(df_children, df_adults)

"""Merge both datasets"""

def merge_datasets(df1, df2):
    try:
        merged_data = pd.concat([df1, df2], ignore_index=True)
        print("Datasets concatenated successfully.")
        return merged_data
    except ValueError as e:
        print("Error:", e)
        print("Column mismatch detected. Check column names and data types in both datasets.")
        return None

# Merge datasets
merged_data = merge_datasets(df_adults, df_children)


if merged_data is not None:
    print("Merged dataset shape:", merged_data.shape)
    print("Columns in merged dataset:", merged_data.columns)

"""Rename misspelled columns"""

def rename_columns(df):
    renamed_columns = {
        'austim': 'autism',
        'jundice': 'jaundice',
        'contry_of_res': 'country_of_res',

    }
    df = df.rename(columns=renamed_columns)
    return df


merged_data = rename_columns(merged_data)
merged_data.head()

"""Rename misspelled responses"""

def rename_misspelled_responses(df):
    # Rename misspelled responses
    df['relation'].replace('self', 'Self', inplace=True)
    return df

# Call the function with the merged dataset
merged_data = rename_misspelled_responses(merged_data)

"""Check for missing data in merged dataset"""

def check_missing_data(df, dataset_name):
    missing_data = df.isna().sum()
    print(f"Missing data in {dataset_name} dataset:")
    print(missing_data)

# Check for missing data in the merged dataset
check_missing_data(merged_data, "merged")

"""Handling missing data"""

def handle_missing_data(df):
    # Replace '?' with NaN in the 'ethnicity' column
    merged_data['ethnicity'].replace('?', np.NaN, inplace=True)

    # Replace '?' with NaN in the 'relation' column
    merged_data['relation'].replace('?', np.NaN, inplace=True)

    # Replace '?' with NaN in the 'age' column
    merged_data['age'].replace('?', np.NaN, inplace=True)

    return merged_data

merged_data = handle_missing_data(merged_data)

"""Imputing missing values of categorical features with mode"""

def replace_non_numeric_with_nan(df):
    df['age'] = pd.to_numeric(df['age'], errors='coerce')
    return df

# Replace non-numeric values with NaN in the merged dataset
merged_data = replace_non_numeric_with_nan(merged_data)

"""Imputing missing non-numerical values with mean"""

numerical_features = ['age', 'result']

def impute_missing_with_mean(merged_data, numerical_features):
    """
    Impute missing values of numerical features in the DataFrame with mean.

    """

    imputer_mean = SimpleImputer(missing_values=np.NaN, strategy='mean')
    merged_data[numerical_features] = imputer_mean.fit_transform(merged_data[numerical_features])


impute_missing_with_mean(merged_data, numerical_features)

"""Finding Outliers in both datasets"""

def find_outliers_age(df, dataset_name):
    max_age = df['age'].max()
    min_age = df['age'].min()

    print(f'Maximum age in {dataset_name} dataset:', max_age)
    print(f'Minimum age in {dataset_name} dataset:', min_age)

# Outliers for Age in Children
find_outliers_age(merged_data, "Merged Dataset")

# Removing row with maximum age from merged dataset
max_age_row_index = merged_data['age'].idxmax()
print("Removing row:", max_age_row_index, "as the respondent has an age of", merged_data.loc[max_age_row_index, 'age'], "which is impossible.")
merged_data.drop(index=max_age_row_index, inplace=True)
merged_data.reset_index(drop=True, inplace=True)

"""Drop Unused columns"""

def drop_unused_columns(df):
    # Drop unused columns
    unused_columns = [ 'id', 'age_desc', 'result']
    df.drop(columns=unused_columns, inplace=True)

    # Print messages about dropped columns
    print("Columns 'id' and 'age_desc' are considered irrelevant and 'age_range' is standard. Dropping them.")

    return df

merged_data = drop_unused_columns(merged_data)
merged_data.head()

"""Analyzing Each Column

Ethnicity
"""

def plot_ethnicity_percentages(df):
    # Group the data by ethnicity and calculate the size of each group
    ethnicity_counts = df.groupby('ethnicity').size()

    # Calculate the total number of cases
    total_cases = ethnicity_counts.sum()

    # Calculate the percentage of cases for each ethnicity
    ethnicity_percentages = (ethnicity_counts / total_cases) * 100

    # Plot the percentages
    plt.figure(figsize=(10, 6))
    ethnicity_percentages.plot(kind='bar', color='skyblue')
    plt.title('Percentage of Cases by Ethnicity')
    plt.xlabel('Ethnicity')
    plt.ylabel('Percentage')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_ethnicity_percentages(merged_data)

"""Conclusion : White Europeans have most cases of ASD around the world
followed by asians.

Gender
"""

def plot_cases_by_gender(df):
    # Group the data by gender and calculate the size of each group
    gender_counts = df.groupby('gender').size()

    # Plot the number of cases by gender
    gender_counts.plot(kind='bar')

    # Adding labels and title to the plot
    plt.xlabel('Gender')
    plt.ylabel('Number of Cases')
    plt.title('Cases by Gender')
    plt.xticks(rotation=360)

    plt.show()

plot_cases_by_gender(merged_data)

"""Conclusion : Males are more prone to ASD in comparision to Females

Jaundice
"""

def plot_cases_by_jaundice(df):
    # Group the data by jaundice and calculate the size of each group
    jaundice_counts = df.groupby('jaundice').size()

    # Plot the number of ASD cases by jaundice
    jaundice_counts.plot(kind='bar')

    # Adding labels and title to the plot
    plt.xlabel('Jaundice')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Jaundice')
    plt.xticks(rotation=360)

    plt.show()

plot_cases_by_jaundice(merged_data)

"""Conclusion : Having jundice does not determine you will have autism.

Relation
"""

def plot_cases_by_relation(df):
    # Aggregate duplicate labels by counting the number of occurrences
    relation_counts = df.groupby('relation').size()

    # Plot the aggregated counts
    relation_counts.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.xlabel('Relation')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Relation')
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
    plt.tight_layout()
    plt.show()

plot_cases_by_relation(merged_data)

"""Conclusion : Most ASD cases around the world do not have their family members with ASD that means ASD is not a genetic disease.

Age
"""

def plot_cases_by_age_range(df):
    # Create bins from 4 to 64 with a step size of 5
    age_bins = np.arange(4, 65, 5)

    # Create age labels for the bins
    age_labels = [f'{age}-{age+4}' for age in age_bins[:-1]]

    # Discretize age into bins and assign labels
    df['age_range'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)

    # Plotting the number of cases by age range
    df.groupby('age_range').size().plot(kind='bar', color='skyblue', edgecolor='black')
    plt.xlabel('Age Range')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Age Range')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_cases_by_age_range(merged_data)

"""Conclusion: Most ASD cases are highest with persons between 4-8 years and 19-23 years

Qchat
"""

def plot_cases_by_score(df):
    # Get the counts of positive scores for each question
    score_counts = df[['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']].sum()

    # Create a bar plot for each score
    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))
    fig.suptitle('ASD Cases by Score', y=1.05)

    for i, (score, ax) in enumerate(zip(score_counts.index, axes.flatten())):
        ax.bar(['Positive', 'Negative'], [score_counts[score], len(df) - score_counts[score]], color=['skyblue', 'lightcoral'])
        ax.set_title(f'Score {i+1}')
        ax.set_ylabel('Number of Cases')

    plt.tight_layout()
    plt.show()

plot_cases_by_score(merged_data)

"""Country"""

def plot_cases_by_country(df):
    # Group the data by country of residence and calculate the size of each group
    cases_by_country = df.groupby('country_of_res').size().sort_values(ascending=False)

    # Plotting the number of cases by country
    plt.figure(figsize=(12, 6))
    cases_by_country.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.xlabel('Country')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Country')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

plot_cases_by_country(merged_data)

"""Conclusion more residents of the United States have more ASD cases.

Used_app_before
"""

def plot_cases_by_used_app_before(df):
    # Group the data by 'used_app_before' and calculate the size of each group
    cases_by_used_app_before = df.groupby('used_app_before').size()

    # Plotting the number of cases by whether the individuals used the app before or not
    plt.figure(figsize=(6, 6))
    cases_by_used_app_before.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.xlabel('Used App Before')
    plt.ylabel('Number of ASD Cases')
    plt.title('ASD Cases by Used App Before')
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

plot_cases_by_used_app_before(merged_data)

"""Feature Relevance"""

# Define features
features_numerical = ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age']
features_categorical = ['gender', 'ethnicity', 'jaundice', 'autism', 'country_of_res', 'relation', 'Class/ASD']

# Combine all features
all_features = features_numerical + features_categorical

# Calculate feature importance or relevance scores (you need to replace this with actual feature importance values)
feature_importance = {feature: importance_score for feature, importance_score in zip(all_features, range(len(all_features)))}

# Define colormap
cmap = plt.get_cmap('viridis')  # You can choose any other colormap

# Plot total cases for each feature with colors based on importance
plt.figure(figsize=(12, 8))
for i, feature in enumerate(all_features):
    total_cases = merged_data.groupby(feature).size()
    color = cmap(feature_importance[feature] / len(all_features))  # Normalize importance to [0, 1] for colormap
    total_cases.plot(kind='bar', color=color, alpha=0.7, label=feature)  # Use alpha to make bars transparent for better visualization
plt.title('Total Cases by Feature (Colored by Importance)')
plt.xlabel('Feature Values')
plt.ylabel('Total Cases')
plt.legend()
plt.xticks(rotation=45)

# Add colorbar
sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=1))  # Normalize colormap to [0, 1]
sm.set_array([])  # Set an empty array since we only want colors
cbar = plt.colorbar(sm)
cbar.set_label('Feature Importance')

plt.tight_layout()
plt.show()

# Print out relevant features and their importance scores
print("Relevant features and their importance scores:")
for feature, importance_score in feature_importance.items():
    print(f"Feature: {feature}, Importance Score: {importance_score}")

"""Extracting Features"""

def split_data(merged_data):
    # Extract features and target label
    features = merged_data.drop(columns=['Class/ASD'])
    target_label = merged_data['Class/ASD']

    return features, target_label

features, target_label = split_data(merged_data)

"""Feature Scaling"""

def min_max_scale_numerical_features(features, features_numerical):
    # Initialize MinMaxScaler
    scaler = MinMaxScaler()

    # Apply Min-Max scaling to numerical features
    features_minmax_transform = pd.DataFrame(data = features)
    features_minmax_transform[features_numerical] = scaler.fit_transform(features[features_numerical])

    return features_minmax_transform

features_numerical = ['age']
features_minmax_transform = min_max_scale_numerical_features(features, features_numerical)

print("Example of records with scaling applied:")
display(features_minmax_transform.head(n=5))

"""Label Encoding"""

def encode_categorical_columns(features_minmax_transform):
    """
    Encodes categorical columns in the features_minmax_transform DataFrame using label encoding.

    """
    le = LabelEncoder()
    for column in features_minmax_transform.columns:
        if pd.api.types.is_numeric_dtype(features_minmax_transform[column]):
            continue
        features_minmax_transform[column] = le.fit_transform(features_minmax_transform[column])
    return features_minmax_transform

# Encode the 'Class/ASD' target label to numerical values (0 and 1)
merged_data_classes = target_label.apply(lambda x: 1 if x == 'YES' else 0)

features_minmax_transform = encode_categorical_columns(features_minmax_transform)
features_final = features_minmax_transform

# Display the first 5 rows of encoded features
print("One-hot encoded features:")
display(features_final.head(5))

# Print the number of features after one-hot encoding
encoded_features_count = len(features_final.columns)
print(f"{encoded_features_count} total features after one-hot encoding.")

print(features_final.columns.tolist())

"""Checking if categorical variables have been converted into numerical features, and all numerical features have been normalized"""

def plot_class_histogram(merged_data_classes):
    # Plot histogram with 10 bins
    plt.hist(merged_data_classes, bins=10)

    # Set x-axis limit from 0 to 1
    plt.xlim(0, 1)
    plt.title('Histogram of Class/ASD')
    plt.xlabel('Class/ASD from processed data')
    plt.ylabel('Frequency')
    plt.show()

# Call the function with the 'Class/ASD' target label
plot_class_histogram(merged_data_classes)

"""Splitting datasets"""

def split_data(features_final, merged_data_classes):
    # Split combined dataset into features (X) and target variable (y)
    X = features_final
    y = merged_data_classes

    # Perform randomization and split into train, validation, and test sets
    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, stratify=y_train_val, random_state=42)

    return X_train, X_val, X_test, y_train, y_val, y_test

# Call the function to split the data
X_train, X_val, X_test, y_train, y_val, y_test = split_data(features_final, merged_data_classes)

print("Training set shape:", X_train.shape, y_train.shape)
print("Validation set shape:", X_val.shape, y_val.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

# Check data types for DataFrame columns
print("Data types:")
print("X_train:")
print(X_train.dtypes)
print("\ny_train:")
print(y_train.dtypes)
print("\nX_val:")
print(X_val.dtypes)
print("\ny_val:")
print(y_val.dtypes)

# Check data types
print("Data types:")
print("X_train:")
print(X_train.dtypes)
print("y_train:")
print(y_train.dtypes)
print("X_val:")
print(X_val.dtypes)
print("y_val:")
print(y_val.dtypes)

# Check for non-numeric values
def check_non_numeric(data, name):
    non_numeric_count = np.sum(np.isnan(data) | np.isinf(data))
    print(f"Non-numeric values in {name}: {non_numeric_count}")

check_non_numeric(X_train, "X_train")
check_non_numeric(X_val, "X_val")
check_non_numeric(y_train, "y_train")
check_non_numeric(y_val, "y_val")

# Check shapes
print("Shapes:")
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)

"""Saving data files"""

def save_data(X_train, X_test, y_train, y_test):
    """
    Save both X_test and y_test into one file, and X_train and y_train into one file.

    """
    # Concatenate X_test and y_test along columns axis
    test_data = pd.concat([X_test, y_test], axis=1)

    # Concatenate X_train and y_train along columns axis
    train_data = pd.concat([X_train, y_train], axis=1)

    # Save concatenated data to CSV files inside the data folder
    test_data.to_csv("/content/data/test_data.csv", index=False)
    train_data.to_csv("/content/data/train_data.csv", index=False)

    print("Data saved to data/test_data.csv and data/train_data.csv files.")

save_data(X_train, X_test, y_train, y_test)